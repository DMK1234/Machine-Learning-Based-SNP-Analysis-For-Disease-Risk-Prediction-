import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import shap
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.decomposition import TruncatedSVD, PCA
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import xgboost as xgb
from collections import defaultdict
from scipy.sparse import hstack
from sklearn.metrics import precision_recall_fscore_support

# Load datasets
phewas_df = pd.read_csv(r'C:\Users\Navya Vanka\Downloads\PheWAS catalog.csv')

# Drop NaN values in critical columns
phewas_df.dropna(subset=['snp', 'phewas phenotype'], inplace=True)

# Sample 50% of data for consistency
phewas_df = phewas_df.sample(frac=0.5, random_state=42).reset_index(drop=True)

# Merge diet recommendations based on disease name
phewas_df = phewas_df.merge(diet_df, left_on='phewas phenotype', right_on='Diseases', how='left')

# One-hot encoding for SNPs
snp_encoder = OneHotEncoder(sparse_output=True)
encoded_snps = snp_encoder.fit_transform(phewas_df[['snp']])

phenotype_encoder = OneHotEncoder(sparse_output=True)
encoded_phenotypes = phenotype_encoder.fit_transform(phewas_df[['phewas phenotype']]).toarray()

# Scale additional numerical features
scaler = StandardScaler()
scaled_values = scaler.fit_transform(phewas_df[['p-value', 'odds-ratio']])
scaled_values_sparse = hstack([encoded_snps, scaled_values])

# Dimensionality reduction using Truncated SVD
svd = TruncatedSVD(n_components=15, random_state=42)
X_reduced = svd.fit_transform(scaled_values_sparse)

# Target variable
y = encoded_phenotypes[:, 0]

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_reduced, y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# XGBoost Classification Model
xgb_clf = xgb.XGBClassifier(objective='binary:logistic', n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42)
xgb_clf.fit(X_train, y_train)

# Predict risk category
y_pred_prob = xgb_clf.predict_proba(X_test)[:, 1]
y_pred = (y_pred_prob > 0.6).astype(int)

# Evaluate model
precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')
print(f"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}")

# Feature importance analysis (Fixing feature name mismatch)
num_svd_components = X_reduced.shape[1]
svd_feature_names = [f'SVD Component {i+1}' for i in range(num_svd_components)]

# Ensure lengths match before creating DataFrame
if len(svd_feature_names) == len(xgb_clf.feature_importances_):
    feature_importance_df = pd.DataFrame({'Feature': svd_feature_names, 'Importance': xgb_clf.feature_importances_})
    top_10_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(10)
    print("Top 10 Most Influential Features:")
    print(top_10_features)
else:
    print(f"Feature name length: {len(svd_feature_names)}, Importance length: {len(xgb_clf.feature_importances_)}")
    raise ValueError("Feature name and importance lengths do not match!")


# SHAP analysis for explainability
explainer = shap.Explainer(xgb_clf)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

# Predict onset ages
rng = np.random.default_rng(42)
phewas_df['age_at_diagnosis'] = rng.integers(30, 80, phewas_df.shape[0])

# Prepare dataset for XGBoost Regression
X_age = X_reduced
y_age = phewas_df['age_at_diagnosis']

X_train_age, X_test_age, y_train_age, y_test_age = train_test_split(X_age, y_age, test_size=0.2, random_state=42)

xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=300, learning_rate=0.04, max_depth=6, gamma=0.1, random_state=42)
xgb_reg.fit(X_train_age, y_train_age)

# Predict onset ages
phewas_df['predicted_onset_age'] = xgb_reg.predict(X_age).round().astype(int)

# Risk categorization
y_pred_prob_full = np.zeros(len(phewas_df))
y_pred_prob_full[:len(y_pred_prob)] = y_pred_prob
phewas_df['risk_category'] = np.where(y_pred_prob_full > 0.4, 'High Risk', 'Low Risk')

# Clustering SNPs using PCA + KMeans
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_reduced)

kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
snp_clusters = kmeans.fit_predict(X_pca)
phewas_df['snp_cluster'] = snp_clusters

# Transition Matrix for Markov Model
transition_matrix = np.zeros((5, 5))
snp_disease_map = defaultdict(set)
for _, row in phewas_df.iterrows():
    snp_disease_map[row['snp_cluster']].add(row['phewas phenotype'])

for cluster_i in range(5):
    for cluster_j in range(5):
        if cluster_i != cluster_j:
            shared_diseases = snp_disease_map[cluster_i] & snp_disease_map[cluster_j]
            if len(shared_diseases) > 0:
                transition_matrix[cluster_i, cluster_j] = len(shared_diseases)

transition_matrix = transition_matrix / transition_matrix.sum(axis=1, keepdims=True)

# Visualize Transition Probabilities
plt.figure(figsize=(8, 6))
sns.heatmap(transition_matrix, annot=True, cmap='Blues', fmt='.2f', xticklabels=range(5), yticklabels=range(5))
plt.title("Markov Model Transition Probabilities")
plt.xlabel("Next SNP Cluster")
plt.ylabel("Current SNP Cluster")
plt.show()

# Simulate Disease Progression
def simulate_disease_progression(start_cluster, steps=10):
    current_cluster = start_cluster
    history = [current_cluster]
    for _ in range(steps):
        next_cluster = np.random.choice(5, p=transition_matrix[current_cluster])
        history.append(next_cluster)
        current_cluster = next_cluster
    return history

# Example Simulation
start_cluster = 0
progression = simulate_disease_progression(start_cluster)
print("Simulated Disease Progression:", progression)

# Onset Age Distribution Plot
plt.figure(figsize=(8, 5))
sns.histplot(phewas_df['predicted_onset_age'], bins=20, color='purple')
plt.xlabel('Predicted Onset Age')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Onset Ages')
plt.show()

# Elbow Method for KMeans Clustering
wcss = []
cluster_range = range(1, 11)

for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_pca)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(cluster_range, wcss, marker='o', linestyle='--', color='b')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.title('Elbow Method for Optimal K')
plt.xticks(cluster_range)
plt.show()
